{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making an LSTM from scratch\n",
    "\n",
    "### To start you will be filling in the blanks to create your own LSTM. \n",
    "### Here are the equations you will need:\n",
    "\n",
    "$f_t=σ(W_{fx}\\cdot x_t+W_{fh}\\cdot h_t+b_f)$\n",
    "\n",
    "$i_t=σ(W_{ix}\\cdot x_t+W_{ih}\\cdot h_t+b_i)$\n",
    "\n",
    "$\\tilde c_t=tanh(W_{cx}\\cdot x_t+W_{ch}\\cdot h_t+b_c)$\n",
    "\n",
    "$o_t=σ(W_{ox}\\cdot x_t+W_{oh}\\cdot h_t+b_o)$\n",
    "\n",
    "$C_t=f_t∗C_t+i_t∗\\tilde c_t$\n",
    "\n",
    "$h_t=o_t∗tanh(C_t)$\n",
    "\n",
    "### and here are the matrix and vector sizes you will need to know:\n",
    "\n",
    "$H$ = size of hidden state, $I$ = size of input features\n",
    "\n",
    "$f_t,i_t,\\tilde c_t,o_t,h_t,C_t = H \\times 1$ \n",
    "\n",
    "$x_t = I \\times 1$\n",
    "\n",
    "$W_{fx},W_{ix},W_{cx},W_{ox} = H \\times I$\n",
    "\n",
    "$W_{fh},W_{ih},W_{ch},W_{oh} = H \\times H$\n",
    "\n",
    "$b_{f},b_{i},b_{c},b_{o} = H \\times 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self,inputsz,hiddensz):\n",
    "        super().__init__()\n",
    "        self.inputsz=inputsz\n",
    "        self.hiddensz=hiddensz\n",
    "        \n",
    "        #input gate\n",
    "        self.Wix =  Parameter(torch.Tensor()) ##################### TO DO #####################\n",
    "        self.Wih = Parameter() ##################### TO DO #####################\n",
    "        self.bi = Parameter() ##################### TO DO #####################\n",
    "        \n",
    "        #forget gate\n",
    "        ##################### TO DO #####################\n",
    "        #c gate\n",
    "        ##################### TO DO #####################\n",
    "        #output gate\n",
    "        ##################### TO DO #####################\n",
    "        \n",
    "        \n",
    "        #paramter initialization\n",
    "        for p in self.parameters():  \n",
    "            if p.data.ndimension()>=2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                nn.init.zeros_(p.data)\n",
    "                \n",
    "    def forward(self,x,initstates=None):\n",
    "        m = len(x)\n",
    "        hidden_seq = []\n",
    "        if initstates is None:\n",
    "            ht,ct = ##################### TO DO #####################\n",
    "        else:\n",
    "            ht,ct = initstates\n",
    "        for t in range(m):\n",
    "            xt = x[t,:].reshape(-1,1)\n",
    "            it =   ##################### TO DO #####################\n",
    "            ft =   ##################### TO DO #####################\n",
    "            gt =   ##################### TO DO #####################\n",
    "            ot =   ##################### TO DO #####################\n",
    "            ct =   ##################### TO DO #####################\n",
    "            ht =   ##################### TO DO #####################\n",
    "            hidden_seq.append(ht.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq,dim=0)\n",
    "        return hidden_seq, (ht,ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(Dataset):\n",
    "\n",
    "    def __init__(self, x, transform=None):\n",
    "        self.data = x\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Addition\n",
    "\n",
    "### First we are going to be teaching the LSTM to do addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addData(size):\n",
    "    size = np.random.choice(range(size,size+int(size/10)))\n",
    "    inputs = torch.Tensor([[np.random.uniform(-.5,.5),0] for i in range(size)])\n",
    "    choice1 = np.random.choice(range(1,10))\n",
    "    choice2 = list(range(1,int(size/2)))\n",
    "    choice2.remove(choice1)\n",
    "    choice2 = np.random.choice(choice2)\n",
    "    inputs[choice1,1]=1\n",
    "    inputs[choice2,1]=1\n",
    "    label = inputs[:,0][inputs[:,1]==1].sum()\n",
    "    return inputs,label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here's a look at the data. The goal is to learn to add the numbers in the left row that have a corresponding 1 in the right row while ignoring the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "addData(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 20\n",
    "\n",
    "train = customDataset([addData(size) for i in range(20)])\n",
    "train_loader = DataLoader(train,batch_size = 1, shuffle = False)\n",
    "\n",
    "val = customDataset([addData(size) for i in range(20)])\n",
    "val_loader = DataLoader(val,batch_size = 1, shuffle = False)\n",
    "\n",
    "test = customDataset([addData(size) for i in range(100)])\n",
    "test_loader = DataLoader(test,batch_size = 1, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featsz, hiddensz = 2, 1\n",
    "model = LSTM(featsz, hiddensz)\n",
    "# Define hyperparameters\n",
    "n_epochs = 100\n",
    "lr=.02\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay = .0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()\n",
    "    hidden = torch.zeros(hiddensz),torch.zeros(hiddensz)\n",
    "    losslist = []\n",
    "    for d,t in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(d.squeeze(0),hidden)\n",
    "        loss = criterion(output.sum(), t)\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        losslist.append(loss.item())\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    validationlist = []\n",
    "    for d,t in val_loader:\n",
    "        output,hidden = model(d.squeeze(0))\n",
    "        loss = criterion(output.sum(), t)\n",
    "        validationlist.append(loss.item())\n",
    "    currval = np.mean(validationlist)\n",
    "    print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "    print(\"training: {:.4f},val Loss: {:.4f}\".format(np.mean(losslist),currval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "labels = []\n",
    "for d,t in test_loader:\n",
    "    output,hidden = model(d.squeeze(0))\n",
    "    predictions.append(output.sum())\n",
    "    labels.append(t)\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.plot(predictions,'r-')\n",
    "plt.plot(labels,'b-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we'll be trying to learn to multiply. The goal is essentially the same except instead of adding the two numbers we multiply them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multData(size):\n",
    "    size = np.random.choice(range(size,size+int(size/10)))\n",
    "    inputs = torch.Tensor([[np.random.uniform(0,1),0] for i in range(size)])\n",
    "    choice1 = np.random.choice(range(1,10))\n",
    "    choice2 = list(range(1,int(size/2)))\n",
    "    choice2.remove(choice1)\n",
    "    choice2 = np.random.choice(choice2)\n",
    "    inputs[choice1,1]=1\n",
    "    inputs[choice2,1]=1\n",
    "    label = inputs[:,0][inputs[:,1]==1].prod()\n",
    "    return inputs,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 20\n",
    "train = customDataset([multData(size) for i in range(20)])\n",
    "train_loader = DataLoader(train,batch_size = 1, shuffle = False)\n",
    "\n",
    "val = customDataset([multData(size) for i in range(20)])\n",
    "val_loader = DataLoader(val,batch_size = 1, shuffle = False)\n",
    "\n",
    "\n",
    "test = customDataset([multData(size) for i in range(40)])\n",
    "test_loader = DataLoader(test,batch_size = 1, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featsz, hiddensz = 2, 1\n",
    "model = LSTM(featsz, hiddensz)\n",
    "# Define hyperparameters\n",
    "n_epochs = 1000\n",
    "lr=.02\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay = .0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "minimum = 100\n",
    "n_epochs = 100\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()\n",
    "    hidden = torch.zeros(hiddensz),torch.zeros(hiddensz)\n",
    "    \n",
    "    losslist = []\n",
    "    for d,t in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(d.squeeze(0),hidden)\n",
    "        loss = criterion(hidden[0], t)\n",
    "        loss.backward(retain_graph=True)\n",
    "        losslist.append(loss.item())\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    validationlist = []\n",
    "    for d,t in val_loader:\n",
    "        output,hidden = model(d.squeeze(0))\n",
    "        predictions.append(hidden[0].item())\n",
    "        labels.append(t.item())\n",
    "        loss = criterion(hidden[0], t)\n",
    "        validationlist.append(loss.item())\n",
    "    currval = np.mean(validationlist)\n",
    "    print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "    print(\"training: {:.4f},val Loss: {:.4f}\".format(np.mean(losslist),currval))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "labels = []\n",
    "for d,t in test_loader:\n",
    "    output,hidden = model(d.squeeze(0))\n",
    "    predictions.append(hidden[0])\n",
    "    labels.append(t)\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.plot(predictions,'r-')\n",
    "plt.plot(labels,'b-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. Why is addition easy for the LSTM to learn?\n",
    "\n",
    "\n",
    "2. Why is multiplication harder for the LSTM to learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model\n",
    "\n",
    "### Here we'll be teaching a model to produce shakespeare. It's a slow a task so we'll be using a more optimized and slightly changed LSTM to do it. The weight matrices have been stacked to reduce the number of matrix multiplications performed and the output now has its own weight matrix. You just need to fill in the function with the correct part of the gates matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class betterLSTM(nn.Module):\n",
    "    def __init__(self,inputsz,hiddensz):\n",
    "        super().__init__()\n",
    "        self.inputsz=inputsz\n",
    "        self.hiddensz=hiddensz\n",
    "        \n",
    "        #input gate\n",
    "        self.Wx =  Parameter(torch.Tensor(hiddensz*4,inputsz))\n",
    "        self.Wh = Parameter(torch.Tensor(hiddensz*4,hiddensz))\n",
    "        self.Wy = Parameter(torch.Tensor(inputsz,hiddensz))\n",
    "        self.by = Parameter(torch.Tensor(inputsz,1))\n",
    "        self.b = Parameter(torch.Tensor(hiddensz*4,1))\n",
    "        #paramter initialization\n",
    "        for p in self.parameters():\n",
    "            if p.data.ndimension()>=2:\n",
    "                nn.init.xavier_uniform_(p.data)\n",
    "            else:\n",
    "                nn.init.zeros_(p.data)\n",
    "                \n",
    "    def forward(self,x,initstates=None):\n",
    "        m = len(x)\n",
    "        hidden_seq = []\n",
    "        if initstates is None:\n",
    "            ht,ct = torch.zeros(self.hiddensz,1),torch.zeros(self.hiddensz,1)\n",
    "        else:\n",
    "            ht,ct = initstates\n",
    "        xt = x\n",
    "        gates = self.Wx@xt+self.Wh@ht+self.b\n",
    "        it = torch.sigmoid() ##################### TO DO #####################\n",
    "        ft = torch.sigmoid() ##################### TO DO #####################\n",
    "        gt = torch.tanh() ##################### TO DO #####################\n",
    "        ot = torch.sigmoid() ##################### TO DO #####################\n",
    "        ct = ft*ct+it*gt\n",
    "        ht = ot*torch.tanh(ct)\n",
    "        output = self.Wy@ht+self.by\n",
    "        return output, (ht,ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we load the data and transform it into one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('clean_shakespeare.txt','r')\n",
    "text = file.read()\n",
    "letters = 'abcdefghijklmnopqrstuvwxyz '\n",
    "# one hot encode characters\n",
    "char_dict = {}\n",
    "for i,char in enumerate(letters):\n",
    "    vec = torch.zeros(27)\n",
    "    vec[i] += 1\n",
    "    char_dict[char] = vec\n",
    "\n",
    "total_data = 40000\n",
    "\n",
    "D = []\n",
    "for i in range(total_data):\n",
    "    x = text[i]\n",
    "    y = text[i+1]\n",
    "    D.append((char_dict[x],char_dict[y]))\n",
    "\n",
    "\n",
    "featsz, hiddensz = 27, 64\n",
    "model = betterLSTM(featsz, hiddensz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function generates text with our model. You can try different seeds to see what you get. \"rand\" influences randomness in the text generation and \"size\" determines how many characters will be produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 'to be or not to be that is the question'\n",
    "def text_gen(model,seed,rand=5,h_size=32,size=100):\n",
    "    n = len(seed)\n",
    "    one_hot = torch.zeros(n,27)\n",
    "    for i in range(n):\n",
    "        one_hot[i] = char_dict[seed[i]]\n",
    "    hidden = torch.zeros(hiddensz,1),torch.zeros(hiddensz,1)\n",
    "    last_char = ''\n",
    "    for vec in one_hot:\n",
    "        y_hat,hidden = model(vec.unsqueeze(1),hidden)\n",
    "        out = y_hat.permute((1,0))\n",
    "    char = letters[torch.argmax(out)]\n",
    "    seed += char\n",
    "    for i in range(size):\n",
    "        last_char = char\n",
    "        y_hat,hidden = model(char_dict[last_char].unsqueeze(1),hidden)\n",
    "        out = y_hat.permute((1,0))\n",
    "        if last_char == ' ':\n",
    "            best = torch.argsort(out)\n",
    "            best = best[-rand:]\n",
    "            char = np.random.choice(best.squeeze(0))\n",
    "            char = letters[char]\n",
    "        else:\n",
    "            char = torch.argmax(out)\n",
    "            char = letters[char]\n",
    "        seed += char\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = customDataset(D)\n",
    "train_loader = DataLoader(train,batch_size = 1, shuffle = False)\n",
    "lr = .001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We truncated the back propogation to speed things up. You can try different values of truncation to see how it performs. \n",
    "\n",
    "### DB COMMENT: Is this really truncation as we usually understand it?  It looks like we're just ignoring most of the training data via this \"truncation term\"..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "truncate = 32\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()\n",
    "    hidden = torch.zeros(hiddensz,1),torch.zeros(hiddensz,1)\n",
    "    losslist = []\n",
    "    i = 1\n",
    "    loss = 0\n",
    "    for d,t in train_loader:\n",
    "        if i%truncate==0:\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "            hidden = (hidden[0].data,hidden[1].data)\n",
    "        output, hidden = model(d.permute((1,0)),hidden)\n",
    "        loss += criterion(output.permute((1,0)),t.argmax(dim=1))\n",
    "        losslist.append(loss.item())\n",
    "        i += 1\n",
    "    print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "    print(\"training: {:.4f}\".format(np.mean(losslist)))\n",
    "    model.eval()\n",
    "    print(text_gen(model,seed,size=1000,rand=10,h_size=hiddensz))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. What is the purpose of truncated back-prop? \n",
    "\n",
    "\n",
    "2. Say we truncate back-propagation at 50 time steps. Can the cell state carry information from more\n",
    "    than 50 time steps back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
